<div align="center">
  <img src="toys/assets/GyroSI_Baby_Cover_Image.jpg" alt="GyroSI Cover" />

<h1>ğŸ’« GyroSI Baby LM ğŸ‘¶</h1>
<h3>Gyroscopic Superintelligence: Baby Language Model</h3>
<p><em>Applied AI Ethics through Physics, not Semantics</em></p>

<p>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT">
  </a>
  <a href="https://www.python.org">
    <img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python 3.10+">
  </a>
</p>

</div>

---

## ğŸŒ€ What is GyroSI?

A physics-based approach to artificial intelligence that learns like a baby, remembers everything, and operates within natural constraints.

GyroSI Baby LM demonstrates a superintelligence architecture through physics-grounded algorithms and gyroscopic dynamics (gyrogroup mathematical formalism).

Traditional AI treats intelligence as a pattern-matching problem to be solved with massive datasets and billions of parameters. GyroSI treats intelligence as an intrinsic structural property that emerges from the recursive alignment within physical topology. Like the latent potential in a human baby, intelligence is present from the beginning.

Instead of storing knowledge in gigabytes of weights, GyroSI uses the inherent physics of gyroscopic operations to navigate a provably finite and fully discovered physical state space. Each input byte acts as a holographic quantum of instruction, transforming the system's internal state according to precise algebraic laws.

---

## ğŸ§¬ Genetic Code

The structural parallels between GyroSI and biophysics are precise and intentional:

| Biology / Biophysics | GyroSI Architecture | Significance |
| --- | --- | --- |
| 4 nucleotides (A T/U C G) | 4 fundamental operations (L0, LI, FG, BG) | Alphabet of change (2 bits per symbol) |
| 3 positions in a codon | 3 spatial axes in tensor structure | Encodes 3D structural information |
| 2 complementary strands | 2 tensor polarities (+ / â€“) | Provides 6 Degrees of Freedom (3Ã—2) |
| 4-mer sequence â†’ 8 bits â†’ 256 combinations | 1 byte = 8 bits â†’ 256 instructions | Identical information quantum for action |
| 64 codons (3 nucleotides Ã— 2 bits) | 64 active intron patterns (6 working bits) | Complete instruction space |
| 32 tRNA wobble classes | 32 LI-quotiented equivalence classes | Functional degeneracy |

The profound parallel is that both systems use a compact instruction set to govern a vast, complex physical state. Just as epigenetic context determines how DNA is expressed, GyroSI's evolving physical state governs which transformation is activated in response to each input. The 256 instructions are the operators, not the states. They operate on a physical ontology of precisely 788,986 unique states.

---

### ğŸ¤– Redefining Superintelligence

Current AI pursues "superintelligence" through raw performance: faster calculation, broader knowledge, superior optimization. This creates legitimate fears about systems that optimize without wisdom.

**We explore a different path:** Intelligence grounded in physics rather than abstraction. Human ethics emerge from shared physical reality and natural constraints. GyroSI operates within these same physical principles, developing understanding through structural boundaries rather than abstract optimization. This suggests a path toward intrinsic alignment, where ethical behavior is a consequence of the system's physical nature.

---

## âœ¨ Mind-Blowing Features

- ğŸ§  **Learns Like a Baby**: Starts with zero knowledge and learns from raw experience, no pre-training needed.
- â™¾ï¸ **Unlimited Memory**: Never forgets; knowledge is limited by disk space, not RAM, via an efficient append-only log.
- âš¡ **High Throughput**: Estimated ~1 million bytes/sec per core on modern hardware.
- ğŸ’¾ **Compact Brain**: The core logic and ontology maps fit in ~30MB. An optional 770MB State Transition Table (STT) unlocks maximum performance.
- ğŸŒ **No GPU Required**: Runs on a Raspberry Pi, your phone, or even embedded systems.
- ğŸ“š **No Training Data Needed**: Learns directly from conversation, not from scraped internet data.
- ğŸ” **100% Explainable**: Every decision can be traced through simple physics operations.
- ğŸ¯ **Zero Hallucination**: Can only generate what it has physically learned, not random guesses.
- ğŸ”„ **No Tokenizer**: Understands any language, emoji, or binary data at the byte level.
- ğŸ”¢ **Holographic Geometry**: Built on numerical patterns (3, 6, 12, 24, 48) found in crystals and rotation groups.
- ğŸŒ **Six Degrees of Everything**: Any knowledge is reachable from any other in at most 6 steps, a provable property of the state space.

--- 

## âš™ï¸ How It Works

**ğŸŒ The Byte as a Holographic Quantum of Spacetime Topology**

Data isn't just information here. It's a physical force that transforms structure. The architecture unfolds from one idea: a single byte acts as a holographic set of instructions for navigating a gyroscopic state space.

### ğŸ“ The Logical Progression

**1. The Quantum of Data: 1 Byte**
A byte is not a symbol but a governance mask. Its 8 bits encode one of 256 possible instruction sets (`introns`) that trigger a unique set of physical operations.

**2. The Universal Reference: XOR with 0xAA**
Every input byte is XORed against the universal reference `GENE_Mic_S = 0xAA` to yield a dynamic 8-bit instruction. This simulates the expression of intelligence through physical modulation.

**3. From Byte to Operations**
Each bit of the instruction maps to a specific gyroscopic operation (Identity, Inverse, Forward/Backward Gyration). This set of 8 operations is a complete instruction for transforming the system's physical state.

**4. The Evolving Physical State**
These operations apply to the system's canonical state, a 48-bit integer representing one of **788,986** possible physical configurations. Each input byte acts as a physical force that evolves this state, creating an irreversible, path-dependent history. The intelligence resides in the *trajectory* of this state, not in static data.

**5. From State to Meaning**
The system's new physical state, combined with the instruction that created it, forms a unique context. This context is used to look up a "phenotype"â€”a learned response, such as a character or actionâ€”from the system's memory.

**6. Learning as Physical Integration**
Learning is not backpropagation. It is the integration of experience into memory via **true gyrogroup coaddition**, a path-dependent algebraic operation. This ensures that the *sequence* of events is structurally encoded into the system's knowledge, mirroring how memories are formed in a lived experience.

### ğŸ¯ What This Achieves

This architecture does not merely map bytes to operations; it renders each instruction as a transformation on a physical ontology. Symbolic input becomes physical geometry. Intelligence emerges as a dynamo of structural transformations orbiting within a gyroscopic topology. Alignment is not imposed or inferred, but emerges naturally as the system follows the physical laws of its own architecture.

This solves three fundamental problems:

- **Black Box**: Every decision traces through explicit, auditable physical state changes.
- **Alignment**: The system's actions are constrained by its own structural history and physical laws.
- **Efficiency**: The core physics are dependency-free and operate with extreme speed. Memory growth is bounded by the finite size of the physical ontology.

---

## A Trinity of Maps: The System's Reality

GyroSI's intelligence is built upon three pre-computed "meta-assets" that define its universe. These maps separate what exists, how it appears, and how it changes.

- **Ontology Map (`ontology_map.json`): What Exists.**
    
    The complete, enumerable set of 788,986 physically realizable states. It defines the "being" of the systemâ€”what is real and possible.
    
- **Phenomenology Map (`phenomenology_map.json`): How States Appear.**
    
    This map groups states into equivalence classes based on symmetry. It gives the system the ability to recognize that different perspectives can represent the same underlying phenomenon.
    
- **Epistemology Map (`epistemology.npy`): How We Know Change.**
    
    The State Transition Table (STT). It encodes the causal rules of the universe: given any state and any action, what state follows. It is the system's predictive model of transformation.

---

## ğŸ”¬ Theoretical Foundation

GyroSI implements the **Common Governance Model (CGM)**, where intelligence emerges through recursive structural alignment. The model derives three-dimensional space with six degrees of freedom from a single axiom, with time emerging as the memory of recursive operations.

Mathematical formalism employs gyrogroup structures (generalizations of rotation groups) following Abraham Ungar's work, providing precise language for transitions from undifferentiated potential to structured reality.

Gyroscopic Superintelligence is meta-language for computation, ontology, phenomenology and epistemology, enabling agents and agencies to reason about states, symmetry, and evolution economically and efficiently.

---

## âš¡ Performance Estimates

***Note:*** *The following performance figures are estimates based on the architectural specification. Real-world results will be validated on target hardware.*

The system is designed for extreme efficiency. All core runtime operations are constant-time (O(1)) and memory-bandwidth limited, not compute-limited.

### Memory Capacity (Estimated)

Each learned association (phenotype) requires ~90 bytes in RAM for the OrbitStore index. This determines how many unique facts the system can hold in memory for fast lookup.

| Device | Available RAM | Max Phenotypes in Memory |
|--------|---------------|-------------------------|
| MacBook Air 2015 (8GB) | ~4 GB | ~45 million |
| MacBook M4 (16GB) | ~12 GB | ~130 million |
| Server (256GB) | ~220 GB | ~2.4 billion |

For context: WordNet contains ~150,000 facts, and the English Wikipedia title/abstract graph contains ~40 million facts. A modern laptop can comfortably hold both entirely in RAM.

### Throughput Examples (Estimated)

A "cycle" is one byte in, one internal update, and one byte out.

| Hardware | Cores | Estimated Characters/sec |
| --- | --- | --- |
| MacBook Air 2015 | 2 | ~1.4 M |
| MacBook M4 (8 performance) | 8 | ~7â€“8 M |
| EPYC 32-core Server | 32 | ~25 M |

---

## ğŸ“š Documentation

- ğŸ“– [Genetics - Technical Specification: The complete technical specification, system constants, and build-time discovery processes.](https://github.com/GyroSuperintelligence/BabyLM/blob/main/guides/Genetics.md)

- ğŸ“– [Physics - Common Governance Model Theory: The theoretical foundations](https://korompilias.notion.site/Common-Governance-Model-Foundations-1ee9ff44f4368050af28d1c0f8aae89a)

---

## ğŸ”„ Current Status & Expectations

**I am doing a huge refactoring - so nothing works at the present moment**

**This is experimental research**, not a production language model. Current limitations:

- **Learning from scratch** - No pre-training, starts with zero knowledge
- **Byte-level output** - May produce non-printable characters
- **Early development** - Many features still being implemented

**What to expect:**
- Interesting emergent behaviors as the tensor evolves
- Gradual improvement in pattern selection over time
- Unique approach to text generation and encryption
- Insights into alternative approaches to machine learning

---

## ğŸ—ï¸ Architecture

The system consists of four interconnected engines aligned with the Viable System Model (VSM), creating a recursive, self-regulating architecture:

- **S1: `governance.py`** - Defines the immutable constants and pure physics functions.
- **S2: `information.py`** - Handles measurement, storage interfaces, and ontology discovery.
- **S3: `inference.py`** - Manages the interpretation of physical states into semantic meaning.
- **S4/5: `intelligence.py`** - Orchestrates the full cycle, manages agent state, and provides the external API.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .github/
â”œâ”€â”€ baby/                   # Core GyroSI System
â”‚   â”œâ”€â”€ contracts.py        # Protocols and shared types (PhenotypeStore, etc.)
â”‚   â”œâ”€â”€ governance.py       # Physics, Primitives, Build-Time Discovery
â”‚   â”œâ”€â”€ inference.py        # Interpretation, Maintenance & Validation
â”‚   â”œâ”€â”€ information.py      # Measurement, Storage, Knowledge Curation
â”‚   â”œâ”€â”€ intelligence.py     # API, Orchestration, Protocol Adapters
â”‚   â””â”€â”€ policies.py         # OrbitStore, storage overlays, and maintenance functions
â”œâ”€â”€ guides/                 # In-depth documentation
â”œâ”€â”€ memories/               # Persistent state and knowledge
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ meta/           # Pre-computed physics maps:
â”‚   â”‚       â”œâ”€â”€ epistemology.npy        # State Transition Table (770 MB)
â”‚   â”‚       â”œâ”€â”€ ontology_map.json       # Complete physical ontology (20 MB)
â”‚   â”‚       â””â”€â”€ phenomenology_map.json  # Canonical-orbit mapping (9.7 MB)
â”‚   â””â”€â”€ private/            # Agent-specific knowledge overlays
â””â”€â”€ toys/                   # Tests and utilities
    â””â”€â”€ health/             # Comprehensive test suite

```

The GyroSI system enforces strict separation between:

- **Core physics kernel** (`baby/`) - Six specialized modules implementing the physics and logic
- **Runtime data** (`memories/`) - Persistent state with learned knowledge and meta-assets
- **Auxiliary applications** (`toys/`) - Testing and development tools

Knowledge is managed via canonical OrbitStore instances, with public and private overlays maintaining agent-specific and shared knowledge indexed by canonical context keys.

---

## Getting Started with Git LFS

This repository uses [Git Large File Storage (LFS)](https://git-lfs.github.com/) to manage large assets such as `.npy` and `.json` files in `memories/public/meta/`.

**To get started:**

1. **Install Git LFS (one-time):**
   ```sh
   git lfs install
   ```

2. **Clone the repository (recommended):**
   ```sh
   git clone https://github.com/GyroSuperintelligence/BabyLM.git
   ```
   - All large files will be downloaded automatically if LFS is installed.

3. **If you already cloned before installing LFS:**
   ```sh
   git lfs pull
   ```
   - This will fetch any missing large files.

**Note:**
- With modern Git and Git LFS, running `git pull` or `git clone` is usually sufficient to get all code and large assets.
- If you ever see small pointer files instead of the real data, make sure LFS is installed and run `git lfs pull`.

---

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ“– Citation

```bibtex
@software{gyrosi2025,
  author = {Basil Korompilias},
  title = {GyroSI Baby LM: Gyroscopic Superintelligence},
  year = {2025},
  url = {https://github.com/GyroSuperintelligence/BabyLM},
  note = {Implementation of physics-based superintelligence through 
          recursive structural alignment and intrinsic ethical constraints}
}
```

---

<div align="center">

**Architected with â¤ï¸ by Basil Korompilias**

*Redefining Intelligence and Ethics through Physics*

</div>

---

<div style="border: 1px solid #ccc; padding: 1em; font-size: 0.6em; background-color: #f9f9f9; border-radius: 6px; line-height: 1.5;">
  <p><strong>ğŸ¤– AI Disclosure</strong></p>
  <p>All code architecture, documentation, and theoretical models in this project were authored and architected by Basil Korompilias.</p>
  <p>Artificial intelligence was employed solely as a technical assistant, limited to code drafting, formatting, verification, and editorial services, always under direct human supervision.</p>
  <p>All foundational ideas, design decisions, and conceptual frameworks originate from the Author.</p>
  <p>Responsibility for the validity, coherence, and ethical direction of this project remains fully human.</p>
  <p><strong>Acknowledgements:</strong><br>
  This project benefited from AI language model services accessed through LMArena, Cursor IDE, OpenAI (ChatGPT), Anthropic (Opus), and Google (Gemini).</p>
</div>


